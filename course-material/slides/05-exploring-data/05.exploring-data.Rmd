---
title: "FIN7028: Times Series Financial Econometrics 3" 
subtitle: "Exploring data"
author: "Barry Quinn"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: true
    css: ["xaringan-themer.css","../mycssblend.css"]
    lib_dir: libs
    nature:
      self_contained: true
      countdown: 150000
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "https://platform.twitter.com/widgets.js"
    seal: true 
---
```{r child = "../setup.Rmd"}
```

```{r setup1, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
library(fontawesome) 
library(xaringanExtra)
library(xaringanthemer)
library(fpp2)
library(tidyverse)
library(tidyquant)
library(knitr)
library(kableExtra)
style_mono_accent(
  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
  )
load("~/Dropbox/Teaching/TSFE/data/fin7028_dat.RData")
```

.acid[
.hand[Learning Outcomes]
- Visualise arguments with data
- Access data programmatically
- Visual trends and patterns
- Criticise poor visualisations
]

---
class: middle

# Time series in R

## Rethinking visualisation

  - Charts are not meant to be *seen*, they are intended to be *read*
  - They are not just images but **visual arguments**.
  - Data doesn't *speak for itself*.
  - Data visualisations need to be **shown** and **explained**

---
class: middle

## `ts` objects and `ts` function

A time series is stored in a `ts` object in R:

 - a list of numbers
 - information about times those numbers were recorded.

### Example

```{r tstable, echo=FALSE, cache=TRUE}
x <- c(123,39,78,52,110)
yr <- 2012:2016
knitr::kable(data.frame(Year=yr,Observation=x), booktabs=TRUE)
```

```r
y <- ts(c(123,39,78,52,110), start=2012)
```

---
class: middle

## `ts` objects and `ts` function

For observations that are more frequent than once per year, add a `frequency` argument.

E.g., monthly data stored as a numerical vector `z`:

```r
y <- ts(z, frequency=12, start=c(2003, 1))
```

## `ts` objects and `ts` function

### `ts(data, frequency, start)`

\begin{tabular}{lrl}
\bf Type of data & \hspace*{1.95cm}\bf frequency                 & \bf start example\hspace*{0.25cm} \\
\midrule
Annual    & 1 & 1995\\
Quarterly & 4 & c(1995,2)\\
Monthly   & 12  & c(1995,9)\\
Daily     & 7 \emph{or} 365.25  & 1 \emph{or} c(1995,234) \\
Weekly    & 52.18  & c(1995,23)\\
Hourly    & 24 \emph{or} 168 \emph{or} 8,766  & 1 \\
Half-hourly  & 48 \emph{or} 336 \emph{or} 17,532 & 1 \\
\end{tabular}

---
class: middle

## Tidyquant package

![](https://youtu.be/woxJZTL2hok)

## Class package (pre-loaded in Q-RaP)

`library(tidyverse)
library(tidyquant)
`
`tidyquant` loads:

* **tq_get** function (for getting data from online sources)
* **tq_transmute** function (for transforming between time frequencies)
* **tq_mutate** function (create return series)

`tidyverse` loads many packages include:

  * **ggplot** plotting package
  * **dplyr** data wrangling package
  
## Class package
```
library(fpp2)
```
This loads:

* **forecast** package (for forecasting functions)
* **fma** package (for lots of time series data)
* **expsmooth** package (for more time series data)

---
class: middle

## Class data

\fontsize{10}{14}\sf

```{r loaddata,eval=F,echo=T}
MyDeleteItems<-ls() 
rm(list=MyDeleteItems) 
# Good practice to clear all objects before loading data
load(url("http://www.barryquinn.com/data/fin7028_dat.RData"))
ls()
```


---
class: middle

## Programmatically accessing data from the `r fa(name = 'cloud',fill = 'lightblue')`

* Download data using `tg_get`
  * Create a monthly series using `tq_transmute`

```{r, echo=TRUE,eval=FALSE}
ftse<-tq_get("^FTSE",from="2016-01-01")
ftse_m<-tq_transmute(ftse,
                     select = adjusted,
                     mutate_fun = to.monthly)
ftse_m_ts<-ts(ftse_m$adjusted, start=c(2016,1), freq=12)
```

## UK Stock market time series

`ts` object has plotting functionality.

```{r, echo=TRUE, fig.height=5}
autoplot(ftse_m_ts) + theme_tq()
```


---
class: middle

## Quarterly earnings time series
\fontsize{10}{14}\sf

`ts` object have print functionality.

```{r echo=TRUE}
carnival_eps_ts
```

## rethinking visualisation using `ggplot2` 

  * `ggplot2` is based on [The Grammar of Graphics](http://amzn.to/2ef1eWp), the idea
that you can build every graph from the same
components: a data set, a coordinate system,
and geoms—visual marks that represent data points.

\includegraphics{ggplot2}

## Example: Exchange rate time series

\small

```{r echo=TRUE, eval= F}
usuk_rate %>%  # Data
  ggplot(aes(x=date, y=price )) + # Coordinate system
  geom_line(colour="pink") # geom
```


## Exchange rate time series

\includegraphics{cable.png}

## Your turn

* Use `tq_get` to download the CBOE VIX Index from `2016-01-01` using the symbol `^VIX`
* create a time series object using the VIX index price.
* Plot this daily VIX Index Price using `autoplot`.

\bigskip

  **Hint:** the ts object of daily financial time series does not have a regular frequency to input into `ts()` function, so leave this argument blank.

# Distributions properties of asset returns

## Why normal?

* This is a plot of a normal distrubtion with mean equal to zero and variance equal to one.

```{r normalplot, echo=FALSE,out.height="70%",fig.align="center"}
require(grDevices) 
require(graphics)
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 0, sd =1)
                ,lwd=2,colour="red") + 
  ylab("") +
  scale_y_continuous(breaks = NULL) +
  labs(title=expression(y[it] %~% N(0,1)),
       x=expression(y[it])) +
  theme(plot.title = element_text(hjust = 0.5,size=20))
```

## Why normal?

* Named after Carolo Friderico Gavss, the normal (or Gaussian) distribution is the most common used distributional assumption in statistical analysis.

\textbf{This is because:}

* Easy to calculate with
* Common in nature
* Very conservative assumptions

## Why normal?

* Suppose 100 students lined up on the midfield line of a soccer pitch.
* Each student flips a fair coin
* Heads they take 1 step to the left
* Tails they take 1 step to the right
* Each student repeats this process 16 times
* What would the the distribution of the position of the students look like after 16 steps?

## Why normal?

\includegraphics{RW.png}

## Why normal?

* Processes that produce normal distributions include:
  * Addition
* Lots of process are approximately normal
  * Product of small deviations
  * logarithms of products
  * Logarithms are just magnitudes

<!-- ## Why normal? -->

<!-- * Ontological perspective -->
<!-- * Processes which add fluctuations results in dampening -->
<!-- * Damped fluctuations end up Gaussian -->
<!-- * No information left, except mean and variance. -->
<!-- * Can't infer process from distribution!  Most do some more science. -->
  
## Why normal?

<!-- * Epistemological perspective -->
* Know only *mean* and *variance*
* Then least suprising and most conservative (*maximum entropy*) distribution is Gaussian.
  * In terms of being conservative, it assumes the least; only the mean and variance. 
* Nature likes maximum entropy distributions.


## Why normal asset returns?
* Financial time series processes considered in this course:
* Return series
* Financial statement information
* Volatility processes
* Extreme events
* Multivariate series
  
## Why assume normal asset returns?
* Normality assumption allows for asset returns properties to be tractable.
* Tractable mean and variance
  * They provide information about the long-term return and risk, respectively.
* Tractable symmetrical properties
  * Symmetry has important implications in holding short and long financial positions in risk management.
* Tractable kurtosis properties 
  * Kurtosis is related to volatility forecasting, efficiency in estimation and tests,etc.

## Our first engineered robot

* To engineer a statistical model of asset returns we need to make some assumptions about the data story or *data generating process*.

$$ \{R_{it}|i-1,\dots N;t=1,\dots,T\} \stackrel{i.i.d}\sim N (m_1,m_2) $$

\bigskip

* A traditional assumption in financial analysis is the simple returns are independently and identically distributed (iid) as normal with a fixed mean $m_1$ and variance $m_2$.

## Our first engineered robot

* The previous assumption is unrealistic in a number of ways:
1. The lower bound of a simple return is -1, but a normal distribution has no lower bound.
2. Multiperiod simple returns $R_it[k]$ is not normally distributed as it is the *product* of one-period returns.
3. Empirically, asset returns tend  to have *heavy tails* or **positive excess kurtosis**
  
## Our second statistical robot: \small{lognormal distribution}

$$ \{r_{it}|i=1,\dots N ;t=1,\dots,T\} \stackrel{i.i.d}\sim N (\mu,\sigma^2) $$

\bigskip

* Another common assumption is that log returns are iid as normal with mean $\mu$ and variance $\sigma^2$.
* As the sum of a finite number of iid random variables is normal, $r_t[k]$ is also normally distributed.
* There is also no lower bound for $r_t$
* However, lognormal assumption is not consistent with **positive excess kurtosis**.

# Empirical properties of log returns

## Are stock returns distributed normally?

\small

* The following code loads Glencore Plc asset prices from yahoo finance, converts daily adjusted prices to monthly log returns, and creates a monthly time series object.

### Ugly code

```{r,echo=T,eval=FALSE}
glen <- tq_get("GLEN.L")
glen_m <- tq_transmute(glen,
                       select = adjusted,
                       mutate_fun = monthlyReturn,
                       type="log",
                       col_rename = "log_return")
glen_m_ts <- ts(glen_m$log_return,
                frequency=12,start=c(2011,5))
```

## Piping (%>%) code better?
### Prettier code
```{r,echo=T,eval=FALSE}
glen <- tq_get("GLEN.L")
glen_m <- glen %>%
  tq_transmute(select = adjusted,
               mutate_fun = monthlyReturn,
               type="log",
               col_rename = "log_return")
glen_m_ts <- glen_m$log_return %>%
  ts(frequency=12,start=c(2011,5))
```

## Are stock returns distributed normally?

  **Visual Evidence**

  * We can use `ggplot` to visualise the empirical distribution, superimposing what the returns would look like if they were normally distributed.

  * **Only two parameters (a mean and a variance) are required to create a hypothetical normal distribution of a returns series**
  
## Are stock returns distributed normally?

```{r,eval=FALSE, echo=TRUE}
glen_m %>% 
  ggplot(aes(x=log_return)) +
  geom_density() +
  stat_function(
    fun=dnorm,
    args=list(mean(glen_m$log_return),
              sd=sd(glen_m$log_return)),
    col="red")
```

## Are stock returns distributed normally?

\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{glen_dist.png}

## Inference

  * **What patterns are revealed?**

\bigskip

  * The normal distribution is superimposed over the histogram of the daily equity returns.
  * Compared to the normal the distribution of the returns has longer tails and a higher central peak.
  * In statistical terms we say the distribution is leptokurtic, or fat-tailed.

## Are stock returns distributed normally?
  
```{r qqplot,echo=FALSE}
glen_m %>%
  ggplot(aes(sample=100*log_return)) +
  stat_qq(distribution = stats::qnorm) +
  stat_qq_line(distribution = stats::qnorm,
               colour="red") +
  labs(title = "Quantile-quantile plot of glencore stock returns")
```

## Inference

* This plot compares the quantiles of a normal distribution (thinner straight line) to the quantiles of the data (thicker scatter plot).
* If the plots exactly overlap then the data is probably normally distributed.
* While the returns and the normal distribution are similar between +12.5% and -12.5%, outside these limits the returns behave non-normally.


## Are stock returns distributed normally?

  **Statistical Evidence**

* Use `table.Stats` from the `tidyquant` package to obtain some summary statistics of the returns.
* Note that a normally distributed series would have the following properties:  
* zero mean, constant variance, zero skewness, and excess kurtosis of four.


## Are stock returns distributed normally ?

\fontsize{8}{14}\sf

```{r basic_stats, echo=FALSE, warning=FALSE}
t<-table.Stats(glen_m$log_return)
t1<-t[c("Observations","NAs","Arithmetic Mean","Skewness","Kurtosis"),]
kable(t1, align = 'c', caption = " Selected summary statistics for monthly Glencore log returns (2016-2020)",booktabs=TRUE)  %>%
  kable_styling(bootstrap_options =c("striped"),latex_options=c("HOLD_position"))
  
```

## Why care about the normal distributed variables?

\fontsize{12}{14}\sf

- In regressions the assumption of normality of model errors is one of the least important
- For the purpose of estimating the regression line it is barely important at all
- Diagnostics of normality of errors is not recommended unless the model is being used to predict individual data points.

## Why care about the normal distributed variables?

\fontsize{8}{14}\sf
- If the distribution of errors is of interest, perhaps because of predictive goals, this should be distinguished from the distribution of the data, $y$.
- **A regression model does not assume or require that predictors are normally distributed**
- Furthermore, the normal distribution on the outcome refers to the regression errors, not the raw data.
- Depending on the structure of the predictors, it is possible for data $y$ to be far from normally distributed even when coming from a linear regression model.
- See Gelman et al., (2020) Chapter 11 for more detail

## Null hypothesis significant testing

\fontsize{12}{14}\sf

* Null hypothesis significance tests (NHST) are models.
* We assume an underlying data story with distributional properties which then allows us to create p-values based on null hypothesis.
* In practice they are often misused to create 'bright line' acceptance or rejection decision about underlying theoretical questions.
* In applied statistics their misuse is well understood

 

## Are stock returns distributed normally?

### Simple hypothesis tests of log returns

\fontsize{8}{14}\sf

$$H_0:\mu=0$$

* In `R` we can use a simple `t.test`


```{r returns_ttest,echo=T}
t.test(glen_m$log_return)
```

## Are stock returns distributed normally?

\fontsize{8}{14}\sf

### Normality test
**Statistical Evidence**

* In `R` we can use `jarque.test` in the `moments` package which is a joint test of the null hypothesis that skewness=0 and kurtosis=3

```{r returns_normality_test ,echo=T}
moments::jarque.test(glen_m$log_return)
```

## Are stock returns distributed normally?

  **Statistical inference**
  
  * Assuming the models underpinning the previous NHST are valid, we can reject the Null that Glencore monthly log returns are normally distributed at even the 1% significance level.

  **Practical inference**
  
  * Assuming normality to model the *middle* of the data is probably ok
  * Modelly extreme observations may need more distributional tools.


## Heavy Tail Statistical Distributions 

Student's t 
  : very similar to the normal but wider and lower.
  
Stable 
  : generalisation of the normal, stable under addition thus can be used with log returns, can capture excess kurtosis well but has infinite variance which conflicts with finance theory.
  
Scale mixture
  : a combination of a number of normals.


# Time plots

## Rethinking time plots

* One **limitation** with time plots is that the simple passage of time is not a good explanatory variable.
* There are occasional exceptions where there is a clear mechanism driving the financial time series.

\bigskip

* **Descriptive chronology is not causal explanation** - Edward Tufte ( 2015) *The visual display of quantitative information* P37

## Time plots

\small

```{r, echo=TRUE, fig.height=4}
glen_m %>%ggplot(aes(x=date,y=log_return)) + 
  geom_line()
```

## Are time plots best?

\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{glen_weekly_seasons.png}

<!-- \includegraphics{glen_weekly_seasons.png} -->

# Seasonal plots

## Seasonal plots

\small 

```{r,echo=T, eval=F}
glen_m_ts %>% ggseasonplot(year.labels=TRUE,
                           year.labels.left=TRUE) + 
  ylab("") +
  ggtitle("Seasonal plot: Glencore returns")
```

## Seasonal plots

\includegraphics{glen_season1.png}

## Seasonal plots

  * Data plotted against the individual "seasons" in which the data were observed.  (In this case a "season" is a month.)
  * Something like a time plot except that the data from each season are overlapped.
  * Enables the underlying seasonal pattern to be seen more clearly, and also allows any substantial departures from the seasonal pattern to be easily identified.
  * In R: `ggseasonplot()`

## Seasonal polar plots

\small

```{r,echo=T,out.height="60%", fig.align="center"}
glen_m_ts %>% ggseasonplot(polar=TRUE) + ylab("")
```

<!-- \includegraphics{glen_season2.png} -->

## Seasonal subseries plots

\small

```{r,echo=T,out.height="60%", fig.align="center"}
ggsubseriesplot(glen_m_ts) + ylab("") +
  ggtitle("Subseries plot: Glencore returns")
```
<!-- \includegraphics{glen_season3.png} -->

## Seasonal subseries plots

  * Data for each season collected together in time plot as separate time series.
  * Enables the underlying seasonal pattern to be seen clearly, and changes in seasonality over time to be visualized.
  * In R: `ggsubseriesplot()`

# Seasonal or cyclic?

## Time series patterns

Trend
  : pattern exists when there is a long-term increase or decrease in the data.

Seasonal
  : pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).

Cyclic
  : pattern exists when data exhibit rises and falls that are \emph{not of fixed period} (duration usually of at least 2 years).

## Time series components

### Differences between seasonal and cyclic patterns:

* seasonal pattern constant length; cyclic pattern variable length
* average length of cycle longer than length of seasonal pattern
* magnitude of cycle more variable than magnitude of seasonal pattern

## Time series patterns

\small

```{r NI house sales}
autoplot(ni_hsales_ts) +
  ggtitle("Northern Ireland Quarter House Sales") +
  xlab("Year") + ylab("Total Verified Sales")
```

## Time series patterns

\small

```{r eps}
autoplot(carnival_eps_ts) +
  ggtitle("Quarterly EPS for Carnival Plc") +
  xlab("Year") + ylab("")
```

## Time series patterns

```{r}
ustb %>%
  filter(date>=date("2010-01-01")) %>%
  ggplot(aes(x=date,y=x4.wk.bank.discount.rate)) +
  geom_line(colour="darkgreen") +
  labs(title=" Time Plot of 1 month Treasury Bill Rate",
       x="",
       y="%")
```

## Seasonal or cyclic?

\alert{Differences between seasonal and cyclic patterns:}

\bigskip

  * seasonal pattern constant length; cyclic pattern variable length
  * average length of cycle longer than length of seasonal pattern
  * magnitude of cycle more variable than magnitude of seasonal pattern

\pause

\begin{alertblock}{}
The timing of peaks and troughs is predictable with seasonal data, but unpredictable in the long term with cyclic data.
\end{alertblock}

# Lag plots and autocorrelation

## Example: Earnings per share

```{r, echo=TRUE, fig.height=6, fig.width=6, out.width="10cm",fig.align="center"}
gglagplot(carnival_eps_ts)
```

## Lagged scatterplots

  * Each graph shows $y_t$ plotted against $y_{t-k}$ for
different values of $k$.
  * The autocorrelations are the correlations associated
with these scatterplots.

## Autocorrelation

**Covariance** and **correlation**: measure extent of **linear relationship** between two variables ($y$ and $X$).\pause

**Autocovariance** and **autocorrelation**: measure linear relationship between **lagged values** of a time series $y$.\pause

We measure the relationship between:

  * $y_{t}$ and $y_{t-1}$
  * $y_{t}$ and $y_{t-2}$
  * $y_{t}$ and $y_{t-3}$
  * etc.

## Autocorrelation

We denote the sample autocovariance at lag $k$ by $c_k$ and the sample autocorrelation at lag $k$ by $r_k$.  Then define

\begin{block}{}
\begin{align*}
c_k &= \frac{1}{T}\sum_{t=k+1}^T (y_t-\bar{y})(y_{t-k}-\bar{y}) \\[0.cm]
\text{and}\qquad
r_{k} &= c_k/c_0
\end{align*}
\end{block}\pause\small

  * $r_1$ indicates how successive values of  $y$  relate to each other
  * $r_2$ indicates how  $y$ values two periods apart relate to each other
  * $r_k$ is \textit{almost} the same as the sample correlation between $y_t$ and $y_{t-k}$.

## Autocorrelation

\small
Results for first 9 lags for Carnival earnings data:

\footnotesize

```{r, echo=FALSE}
epsacf <- matrix(acf(c(carnival_eps_ts), lag.max=9,
                      plot=FALSE)$acf[-1,,1], nrow=1)
colnames(epsacf) <- paste("$r_",1:9,"$",sep="")
knitr::kable(epsacf, booktabs=TRUE,
             align="c", digits=3,
             format.args=list(nsmall=3))
```

```{r epsacf, fig.height=2.5}
ggAcf(carnival_eps_ts)
```

## Autocorrelation

  * $r_{4}$  higher than for the other lags. This is due to **the seasonal pattern in the data**: the peaks tend to be **4 quarters** apart and the troughs tend to be **2 quarters** apart.
  * $r_2$ is more negative than for the other lags because troughs tend to be 2 quarters behind peaks.
  * Together, the autocorrelations at lags 1, 2, \dots, make up the \emph{autocorrelation} or ACF.
  * The plot is known as a **correlogram**

## ACF

```{r, fig.height=4, echo=TRUE}
ggAcf(carnival_eps_ts)
```

## Trend and seasonality in ACF plots

- When data have a trend, the autocorrelations for small lags tend to be large and positive.
- When data are seasonal, the autocorrelations will be larger at the seasonal lags (i.e., at multiples of the seasonal frequency)
- When data are trended and seasonal, you see a combination of these effects.

## Carnival quarterly earnings

```{r}
autoplot(carnival_eps_ts)
```

## Carnival quarterly earnings

```{r}
ggAcf(carnival_eps_ts)
```

## Carnival quarterly earnings

Time plot shows clear trend and seasonality.

The same features are reflected in the ACF.

  * The slowly decaying ACF indicates trend.
  * The ACF peaks at lags 4, 8, 12, 16, 20, \dots, indicate seasonality of length 4.

## Glencore daily stock price

```{r}
glen <- tq_get("GLEN.L")
glen_ts<-ts(glen$adjusted,freq=1)
```



```{r}
autoplot(glen_ts)
```

## Glencore daily stock price

```{r}
ggAcf(glen_ts, lag.max=100)
```

## Your turn

We have introduced the following graphics functions:

  - `gglagplot`
  - `ggAcf`

Explore the following time series using these functions. Can you spot any seasonality, cyclicity and trend? What do you learn about the series?

  - `ni_hsales_ts`
  - `vix_ts`
  - `ftse_m_ts`

## Which is which?

```{r, fig.height=6, fig.width=12, echo=FALSE, warning=FALSE, out.width="11.5cm"}
tp1 <- autoplot(ni_hsales_ts) + xlab("") + ylab("Total Verified Sales") +
  ggtitle("1. NI house sales")
tp2 <- autoplot(vix_ts) + xlab("") + ylab("") +
  ggtitle("2. VIX prices")
tp3 <- autoplot(ftse_m_ts) + xlab("") + ylab("") +
  ggtitle("3. FTSE 100 log returns")
acfa <- ggAcf(ni_hsales_ts, ci=0) + xlab("") + ggtitle("A") + ylim(-0.4,1)
acfb <- ggAcf(vix_ts, ci=0) + xlab("") + ggtitle("B") + ylim(-0.4,1)
acfc <- ggAcf(ftse_m_ts, ci=0) + xlab("") + ggtitle("C") + ylim(-0.4,1)
gridExtra::grid.arrange(tp1,tp2,tp3,
                        acfc,acfb,acfa,nrow=2)
```

## Your turn
  - load `fin7028_dat.RData` from canvas

We have introduced the following graphics functions:

  - `gglagplot`
  - `ggAcf`

Explore the following time series using these functions. Can you spot any seasonality, cyclicity and trend? What do you learn about the series?

  - `ni_hsales_ts`
  - `vix_ts`
  - `ftse_ts`

## Which is which?

```{r, fig.height=6, fig.width=12, echo=FALSE, warning=FALSE, out.width="11.5cm"}
tp1 <- autoplot(ni_hsales_ts) + xlab("") + ylab("Total Verified Sales") +
  ggtitle("1. NI house sales")
tp2 <- autoplot(vix_ts) + xlab("") + ylab("") +
  ggtitle("2. VIX prices")
tp3 <- autoplot(ftse_m_ts) + xlab("") + ylab("") +
  ggtitle("3. FTSE 100 log returns")
acfa <- ggAcf(ni_hsales_ts, ci=0) + xlab("") + ggtitle("A") + ylim(-0.4,1)
acfb <- ggAcf(vix_ts, ci=0) + xlab("") + ggtitle("B") + ylim(-0.4,1)
acfc <- ggAcf(ftse_m_ts, ci=0) + xlab("") + ggtitle("C") + ylim(-0.4,1)
gridExtra::grid.arrange(tp1,tp2,tp3,
                        acfc,acfb,acfa,nrow=2)
```

## Which is which?

1. NI house sales is A ACF plot
2. VIX prices is B ACF plot
3. FTSE 100 log returns is C ACF plot

# White noise

## Example: White noise

```{r}
wn <- ts(rnorm(36))
autoplot(wn)
```

## Example: White noise

```{r, results='asis', echo=FALSE}
wnacf <- matrix(acf(c(wn), lag.max=10,
                      plot=FALSE)$acf[-1,,1], nrow=1)
colnames(wnacf) <- paste("$r_{",1:10,"}$",sep="")
print(xtable::xtable(t(wnacf)),
    sanitize.rownames.function=identity,
    booktabs=TRUE,
    include.colnames = FALSE,
    hline.after = FALSE,
    size='small',
    comment=FALSE,
    floating=FALSE)
```

\centerline\textbf{Sample autocorrelations for white noise series.}

We expect each autocorrelation to be close to zero.

## Sampling distribution of autocorrelations

Sampling distribution of $r_k$ for white noise data is asymptotically N(0,$1/T$).\pause

  *  95% of all $r_k$ for white noise must lie within $\pm 1.96/\sqrt{T}$.
  * If this is not the case, the series is probably not WN.
  * Common to plot lines at $\pm 1.96/\sqrt{T}$ when plotting ACF.
These are the \textcolor{orange}{\textbf{\emph{critical values}}}.

## Autocorrelation

\placefig{5}{1.6}{width=8cm}{wnacf}

\begin{textblock}{4.8}(0.2,1.5)
\structure{Example:}

$T=36$ and so critical values at $\pm
1.96/\sqrt{36} = \pm 0.327$.

All autocorrelation coefficients lie within these
limits, confirming  that the data are white noise. (More precisely, the data cannot be \\
distinguished \rlap{from white noise.)}
\end{textblock}

## Example: US Investor Sentiment

\small

\includegraphics{sentiment.png}

## Example: US Investor Sentiment

```{r}
ggAcf(sentiment_ts)
```

## Example: US Investor Sentiment

\small

The AAII Sentiment Survey measures the percentage of individuals who are bullish, bearish, and neutral about the stock market over the next six months.
(https://www.aaii.com/journal/sentimentsurveyarticle.)

  * Difficult to detect pattern in time plot.
  * ACF shows some significant autocorrelation on all 30 lags.
  * After $r_{15}$ there seems to be a undulating pattern.
  * These show the series is **not a white noise series**.

## Your turn

You can compute the daily changes in the Google stock price using
```r
dgoog <- diff(goog)
```
Does `dgoog` look like white noise?

# Noise in Financial Data

## Fisher Black, “Noise”, Journal of Finance ,41 ,3 (1986):

\hfill

\begin{displayquote}
Noise makes trading in financial markets possible, and thus allows us to observe prices in financial assets. [But] noise also causes markets to be somewhat inefficient…. . Most generally, noise makes it very difficult to test either practical or academic theories about the way that financial or economic markets work. We are forced to act largely in the dark-(p.529)
\end{displayquote} 

## Financial Data Forecastability

  - Predictability of an event or quantity depends on several factors
  1. How well we understand the factors that contribute to it
  2. How much data are available
  3. Whether the forecast can affect the thing we are trying to forecast

<!-- ## Prediction and EMH -->
<!--   - Crudely, the efficent market hypothesis (EMH) implies that returns from speculative assets are *unforecastable* -->
<!--   - The overpowering logic of the EMH is: -->
<!--     - **If returns are forecastable,there should exist a money machine producing unlimited wealth** -->
<!-- - Based on the random walk theory change in price are defined as white noise as follows: -->
<!-- \begin{align*} -->
<!-- p_t=p_{t-1} + a_t \\ -->
<!-- p_t-p_{t-1} = a_t \\ -->
<!-- \text{where} -->
<!-- \end{align*} -->



## Signal and the Noise

  - High quality models identify the signal from the noise in financial data.
  - The **signal** is the regular pattern that is likely to repeat.
  - The **noise** is the irregular pattern which occurrs by chance and unlikely to repeat.
  - Overfitting or data snooping can result in your model capturing both **signal** and **noise**.
  - Overfitted models usually produce poor predictions and inferences.
