---
title: "FIN7030: Times Series Financial Econometrics 3"
subtitle: "Statistical inference and simulation"
author: "Barry Quinn PhD CStat"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer.css","mycssblend.css","slides-style.css"]
    nature:
     countdown: 120000
     highlightStyle: github
     highlightLines: true
     countIncrementalSlides: true
     ratio: "16:9"
     seal: True 
---
```{r child = "../setup.Rmd"}
```

```{r setup1, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE,out.width = "60%")
library(tidyverse)
pacman::p_load('tidyverse','fontawesome','xaringanExtra','xaringanthemer','bayesplot','arm')
use_panelset()
use_tile_view()
style_mono_accent(
  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
  )
```


class:inverse, middle

# Learning Outcomes

.large[
- What is statistical inference?
- Concluding and questions
]

---
class: middle

# Three challenges of statistics

-.acid[Generalising from sample to population]

-.acid[Generalising from treatment to control group]

-.acid[Generalising from observed measurement to the underlying construct of interest]

.blockquote[All three challenges can be framed as problems of prediction
`r tufte::quote_footer("Vehtari,Gelman and Hill, 2021")`(hereafter ROS)]

---
class: middle

# Weapon of choice in social science

.glow[regression]

.blockquote[ Regression is a method that allows researchers to summarisze how predictions or average values of an *outcome* vary across individuals defined by a set of predictors]

---
class: middle

# Regression uses

.acidline[
- Prediction: *Predicting victory or defeat in a sport contest*
- Exploring association: *Summarising how well one variable, or set of variables, predicts outcomes, for example risk factor modelling in asset pricing*
- Extrapolation: *Adjusting for known differences between the sample (observed data) and a population of interest, For example adjusting for Big Data online survey data for response bias*
- Causal inference: *The most important use: estimating treatment effects by comparing outcomes under treatment and control*
]
---
class: middle

# Weapon of choice in social science

.blockquote.large[A key challenge for causal inference is ensuring that treatment and control groups are similar, on average, before exposure to the treatment, or else adjusting for differences between groups- ROS]

---
class: middle

## Challenges in building, understanding, and interpreting regressions

.panelset[
.panel[
.panel-name[Hypothetical example of regression for causal inference]
- Start with a simple scenario comparing treatment and control groups.  
- This condition can be approximated by *randomisation*, a design in which experimental units (in finance we can think of these a firms) are randomly assigned to treatment or control.
- Consider the following hypothetical example where $x$ is a random market shock (the treatment) affecting only certain firms in the UK market (x=0 for control or x=1 for treatment)
]
.panel[.panel-name[Fake data + linear regression with binary predictor]
.pull-left[
```{r}
N <- 50
x <- runif(N, 1, 5)
y <- rnorm(N, 10 + 3*x, 3)
x_binary <- ifelse(x<3, 0, 1)
data <- data.frame(N, x, y, x_binary)
lm_1a<-lm(y~x_binary,data)
display(lm_1a)
```
]
.pull-right[
- If we can assume comparability of the groups assigned to different treatments, a regression predicting the outcome given treatment gives us a direct estimate of the causal effect.
- .acidline[We will come back to the important set of assumptions charges this statistically engineered robot with causal inference power]
- The results opposite show that the treatement as a positive and significant effect on our outcome measure.
]
]
.panel[.panel-name[Visualising data + model]
```{r, include=FALSE }
data %>%
  ggplot(aes(x=x_binary,y=y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x="", y="Outcome measurement",title="Regression with binary treatment") +
  geom_text( aes(x=0.3,y=13,label=paste("Estimated treatment effect is\nslope of fitted line: ", fround(coef(lm_1a)[2], 1)),
            parse = TRUE))

```
]
]
]


---
class: middle

## linear regression with continous predictor

.panelset[
.panel[.panel-name[linear regression output]
```{r}
lm_1b <- lm(y ~ x, data = data)
display(lm_1b)
```

]
.panel[.panel-name[Visualise data and model]

```{r, echo=FALSE}
data %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x="Treatment level", y="Outcome measurement",title="Regression with continuous treatment") +
  geom_text( aes(x=3.2,y=15,label=paste("Estimated treatment\neffect per unit of x is\nslope of fitted line: ", fround(coef(lm_1b)[2], 1)),
            parse = TRUE))

```
]
]

---
class: middle

## Non-linear predictor

.panelset[
.panel[.panel-name[Fake data + regression]
```{r}
y <- rnorm(N, 5 + 30*exp(-x), 2)
data$y <- y
lm_2a <- lm(y ~ x, data = data)
display(lm_2a)
```
]
.panel[.panel-name[Visual data + *true* non-linear effect]
```{r, echo=FALSE}
data %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  #geom_smooth(method = "loess", se = FALSE) +
  labs(x="Treatment level", y="Outcome measurement",title="Nonlinear treatment effect") +
  geom_function(fun=function(x) 5+ 30*exp(-x))
```

]
.panel[.panel-name[Visual data + linear effect model]
```{r, echo=FALSE}
data %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
    labs(x="Treatment level", y="Outcome measurement",title="Nonlinear effect, estimated with straight line fit")
```

]
]

---
class: middle

## Hypothetical causal adjustment example

.panelset[.panel[.panel-name[Fake data with imbalance in groups]
```{r,echo=FALSE}
N <- 100
xx <- rnorm(N, 0, 1)^2
z <- rep(0:1, N/2)
xx <- ifelse(z==0, rnorm(N, 0, 1.2)^2, rnorm(N, 0, .8)^2)
yy <- rnorm(N, 20 + 5*xx + 10*z, 3)
data <- data.frame(xx, z, yy)
lm_2 <- lm(yy ~ xx + z, data=data)
```

```{r,include=FALSE}
data %>%
  group_by(z) %>%
  dplyr::summarise(mean(xx),mean(yy))->means
```

```{r}
display(lm_2)
```
This hypothetical example can be summarised as follows:
.blockquote[On average, the treated units were 5.02 points higher than the control, $\bar{y}$=`r means[2,3]` for the treated and $\bar{y}$=`r means[1,3]` for the controls. But the two groups differed in their pre-treatment predictor: $\bar{x}$=`r means[2,2]` for treated and  $\bar{x}$=`r means[2,2]` for controls.  After adjusting for this difference, we obtained an estimated treatment effect of 10.0]
]
.panel[.panel-name[Visualise data + model]
```{r, echo=FALSE}
x0 <- 5.2
data %>%
  ggplot(aes(x=xx,y=yy,colour=as.factor(z))) +
  geom_point() +
    labs(x="Pre- treatment predictor", y="Outcome measurement",title="Continuous pre-treatment predictor and binary treatment") +
  geom_abline(intercept =coef(lm_2)[1],slope = coef(lm_2)[2] ) +
  geom_abline(intercept=coef(lm_2)[1]+coef(lm_2)[3],slope =coef(lm_2)[2]) +
geom_text(aes(2.3, 29.5, label="Controls")) +
geom_text(aes(1.5, 45, label="Treated")) + 
  geom_text(aes(x0+.1, coef(lm_2)[1] + coef(lm_2)[2]*x0 + .5*coef(lm_2)[3], label=paste("Estimated\ntreatment\neffect is", fround(coef(lm_2)[3], 1)))) +
  geom_segment(aes(x=x0, y=coef(lm_2)[1] + coef(lm_2)[2]*x0, xend=x0, yend=coef(lm_2)[1] + coef(lm_2)[2]*x0 + coef(lm_2)[3]), arrow = arrow(length = unit(0.1, "cm")))
```
]
]

---
class: middle

## Building interpreting and checking regression models

* Model building, starting with simple linear models of the form, $y=a+bx+err0r$ and expanding through additional predictors, interactions, and transformations.

* Model fitting, which includes data manipulation, programming, and the use of algorithms to estimate regression coefficients and their uncertainties and to make probabilistic predictions.

* Understanding model fits, which involves graphics, more programming, and an active investigation of the (imperfect) connections between measurements, parameters, and the underlying objects of study.

• Criticism, which is not just about finding flaws and identifying questionable assumptions, but is also about considering directions for improvement of models. Or, if nothing else, limiting the claims that might be made by a naive reading of a fitted model.

The next step is to return to the model-building step, possibly incorporating new data in this effort. 

---
class: middle

## Classical and Bayesian inference

- As open science econometricians we mostly fit models to data and uses model to predict.
- There are three concerns common to all all stesp in this framework

1. What **information** is used in the estimation process
2. What **assumptions** are made
3. How estimates and predictions are interpreted in a Classical or Bayesian framework

---
class: middle

## Information

- In regressions we usually have data on an outcome 

---
class: middle

## Computing least squares

.panelset[
.panel[.panel-name[Beauty and teaching evaluations]
```{r visual data,echo=FALSE}
library(rosdata)
beauty<-rosdata::beauty
beauty %>%
  dplyr::select(eval,beauty) %>%
  ggplot(aes(y=beauty,x=eval)) +
  geom_point() +
  geom_smooth(method = "lm",se=FALSE)
```

]
.panel[.panel-name[Frequentist inferennce]
Does beauty predict student evaluations?
```{r}
display(lm(eval~beauty,data=rosdata::beauty))
```
]
.panel[.panel-name[Bayesian inference]
```{r bayesian, include=FALSE}
library(rstanarm)
library(bayesplot)
library(tidyverse)
options(mc.cores = parallel::detectCores())
m1<-stan_glm(eval~beauty,data=beauty)
```

```{r, echo=FALSE}
summary(m1, digits = 4)
```
]
.panel[.panel-name[Plotting the uncertainty]
```{r echo=FALSE}
posterior <- as.matrix(m1)
plot_title <- ggtitle("Posterior distributions",
                      "with medians and 80% intervals")
mcmc_areas(posterior,
           pars = c("beauty"),
           prob = 0.8) + plot_title
```

]
]

---
class: middle, center, hide-logo
background-image: url(img/title_slide.png)
background-size: cover

# .acid[Thank You]

# .glow[Questions?]

---
class: middle
### Extra reading (all link to qub library ebooks)

[López de Prado, Marcos. 2020. Machine Learning for Asset Managers. In Elements in Quantitative Finance. Cambridge University Press.]("https://encore.qub.ac.uk/iii/encore/record/C__Rb2203007%C2%A0")

[------. 2018. Advances in Financial Machine Learning. John Wiley & Sons.](https://encore.qub.ac.uk/iii/encore/record/C__Rb2203005)

[Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference. Cambridge University Press.](https://encore.qub.ac.uk/iii/encore/record/C__Rb2203007%C2%A0)

[Dempster, M.A.H., Juho Kanniainen, John Keane, Erik Vynckier. 2018. High-Performance Computing in Finance: Problems, Methods, and Solutions. Cambridge University Press.](https://encore.qub.ac.uk/iii/encore/record/C__Rb2203009)

[Dixon, Matthew F., Igor Halperin, and Paul Bilokon. 2020. Machine Learning in Finance: From Theory to Practice. Springer International Publishing.](https://encore.qub.ac.uk/iii/encore/record/C__Rb2203004)

[Molnar,C.(2019)" Interpretable Machine Learning: A Guide for Making Black Box Models Explainable"](https://encore.qub.ac.uk/iii/encore/record/C__Rb2183044)

[Gelman, A; Hill, J; & Ati Vehtari (2020)., Regression and Other stories, Wiley Publishing.](https://www-cambridge-org.queens.ezp1.qub.ac.uk/highereducation/books/regression-and-other-stories/DD20DD6C9057118581076E54E40C372C#overview)

[Cunningham, S. (2021). Causal inference: The mixtape. Yale University Press.](https://mixtape.scunning.com/)

[Statistical rethinking : a Bayesian course with examples in R and Stan / Richard McElreath](https://encore.qub.ac.uk/iii/encore/record/C__Rb2089842__Sstatistical%20rethinking__Orightresult__U__X7?lang=eng&suite=def)

